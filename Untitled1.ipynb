{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jhsong76/KoBERT/blob/main/Untitled1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uSdMEZCT5ZrG"
      },
      "outputs": [],
      "source": [
        "!pip install mxnet\n",
        "!pip install gluonnlp==0.8.0\n",
        "!pip install tqdm pandas\n",
        "!pip install sentencepiece\n",
        "!pip install transformers\n",
        "!pip install torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install 'git+https://github.com/SKTBrain/KoBERT.git#egg=kobert_tokenizer&subdirectory=kobert_hf'"
      ],
      "metadata": {
        "id": "pRv-EmzwRTdu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import gluonnlp as nlp\n",
        "import numpy as np\n",
        "from tqdm import tqdm, tqdm_notebook\n",
        "# ★ Hugging Face를 통한 모델 및 토크나이저 Import\n",
        "from kobert_tokenizer import KoBERTTokenizer\n",
        "from transformers import BertModel\n",
        "\n",
        "from transformers import AdamW\n",
        "from transformers.optimization import get_cosine_schedule_with_warmup"
      ],
      "metadata": {
        "id": "Yw5EEhbNRXoG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# GPU 사용 시\n",
        "device = torch.device(\"cuda:0\")"
      ],
      "metadata": {
        "id": "Tniw_m_nReIr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_kobert_model(model_path, vocab_file, ctx=\"cpu\"):\n",
        "    bertmodel = BertModel.from_pretrained(model_path)\n",
        "    device = torch.device(ctx)\n",
        "    bertmodel.to(device)\n",
        "    bertmodel.eval()\n",
        "    vocab_b_obj = nlp.vocab.BERTVocab.from_sentencepiece(vocab_file,\n",
        "                                                         padding_token='[PAD]')\n",
        "    return bertmodel, vocab_b_obj\n",
        "import gluonnlp as nlp\n",
        "tokenizer = KoBERTTokenizer.from_pretrained('skt/kobert-base-v1')\n",
        "bertmodel, vocab = get_kobert_model('skt/kobert-base-v1',tokenizer.vocab_file)"
      ],
      "metadata": {
        "id": "E1DjuqPhRgJs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://www.dropbox.com/s/374ftkec978br3d/ratings_train.txt?dl=1\n",
        "!wget https://www.dropbox.com/s/977gbwh542gdy94/ratings_test.txt?dl=1"
      ],
      "metadata": {
        "id": "NutJ8yOxSWN7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import gluonnlp as nlp\n",
        "import numpy as np\n",
        "from tqdm import tqdm, tqdm_notebook\n",
        "import pandas as pd\n",
        "\n",
        "#transformers\n",
        "from transformers import AdamW\n",
        "from transformers.optimization import get_cosine_schedule_with_warmup\n",
        "from transformers import BertModel\n",
        "\n",
        "#GPU 사용 시\n",
        "device = torch.device(\"cuda:0\")\n",
        "# [AI Hub] 감정 분류를 위한 대화 음성 데이터셋\n",
        "data = pd.read_csv(\"/content/5차년도_2차.csv\", encoding='cp949')"
      ],
      "metadata": {
        "id": "WIGX7VPzScRV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 새 섹션"
      ],
      "metadata": {
        "id": "ghzqwVOutNUy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data['상황'].unique()"
      ],
      "metadata": {
        "id": "_tKiMFM3SmNi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 7개의 감정 class → 숫자\n",
        "data.loc[(data['상황'] == \"fear\"), '상황'] = 0  # fear → 0\n",
        "data.loc[(data['상황'] == \"surprise\"), '상황'] = 1  # surprise → 1\n",
        "data.loc[(data['상황'] == \"angry\"), '상황'] = 2  # angry → 2\n",
        "data.loc[(data['상황'] == \"sadness\"), '상황'] = 3  # sadness → 3\n",
        "data.loc[(data['상황'] == \"neutral\"), '상황'] = 4  # neutral → 4\n",
        "data.loc[(data['상황'] == \"happiness\"), '상황'] = 5  # happiness → 5\n",
        "data.loc[(data['상황'] == \"disgust\"), '상황'] = 6  # disgust → 6"
      ],
      "metadata": {
        "id": "CYVt1ZFnSonH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data['상황'].unique()"
      ],
      "metadata": {
        "id": "XaELClXRSqcP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# [발화문, 상황] data_list 생성\n",
        "data_list = []\n",
        "for ques, label in zip (data['발화문'], data['상황']):\n",
        "  data = []\n",
        "  data.append(ques)\n",
        "  data.append(str(label))\n",
        "\n",
        "  data_list.append(data)"
      ],
      "metadata": {
        "id": "rD3aku9SSs9G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(data)\n",
        "print(data_list[:10])"
      ],
      "metadata": {
        "id": "9yWCzw8-Svfo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "dataset_train, dataset_test = train_test_split(data_list, test_size = 0.2, shuffle = True, random_state = 32)"
      ],
      "metadata": {
        "id": "HeYj7WrlSxwg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(dataset_train), len(dataset_test))"
      ],
      "metadata": {
        "id": "Xok_ZAZbSz-N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tok = nlp.data.BERTSPTokenizer(tokenizer, vocab, lower = False)"
      ],
      "metadata": {
        "id": "ibv1ADWES15l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# parameter 값 출처 : https://github.com/SKTBrain/KoBERT/blob/master/scripts/NSMC/naver_review_classifications_pytorch_kobert.ipynb\n",
        "max_len = 64\n",
        "batch_size = 64\n",
        "warmup_ratio = 0.1\n",
        "num_epochs = 5\n",
        "max_grad_norm = 1\n",
        "log_interval = 200\n",
        "learning_rate =  5e-5\n"
      ],
      "metadata": {
        "id": "9lpBwqh6S4Q1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# BERTDataset : 각 데이터가 BERT 모델의 입력으로 들어갈 수 있도록 tokenization, int encoding, padding하는 함수\n",
        "# 출처 : https://github.com/SKTBrain/KoBERT/blob/master/scripts/NSMC/naver_review_classifications_pytorch_kobert.ipynb\n",
        "\n",
        "class BERTSentenceTransform:\n",
        "    r\"\"\"BERT style data transformation.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    tokenizer : BERTTokenizer.\n",
        "        Tokenizer for the sentences.\n",
        "    max_seq_length : int.\n",
        "        Maximum sequence length of the sentences.\n",
        "    pad : bool, default True\n",
        "        Whether to pad the sentences to maximum length.\n",
        "    pair : bool, default True\n",
        "        Whether to transform sentences or sentence pairs.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, tokenizer, max_seq_length,vocab, pad=True, pair=True):\n",
        "        self._tokenizer = tokenizer\n",
        "        self._max_seq_length = max_seq_length\n",
        "        self._pad = pad\n",
        "        self._pair = pair\n",
        "        self._vocab = vocab\n",
        "\n",
        "    def __call__(self, line):\n",
        "        \"\"\"Perform transformation for sequence pairs or single sequences.\n",
        "\n",
        "        The transformation is processed in the following steps:\n",
        "        - tokenize the input sequences\n",
        "        - insert [CLS], [SEP] as necessary\n",
        "        - generate type ids to indicate whether a token belongs to the first\n",
        "        sequence or the second sequence.\n",
        "        - generate valid length\n",
        "\n",
        "        For sequence pairs, the input is a tuple of 2 strings:\n",
        "        text_a, text_b.\n",
        "\n",
        "        Inputs:\n",
        "            text_a: 'is this jacksonville ?'\n",
        "            text_b: 'no it is not'\n",
        "        Tokenization:\n",
        "            text_a: 'is this jack ##son ##ville ?'\n",
        "            text_b: 'no it is not .'\n",
        "        Processed:\n",
        "            tokens: '[CLS] is this jack ##son ##ville ? [SEP] no it is not . [SEP]'\n",
        "            type_ids: 0     0  0    0    0     0       0 0     1  1  1  1   1 1\n",
        "            valid_length: 14\n",
        "\n",
        "        For single sequences, the input is a tuple of single string:\n",
        "        text_a.\n",
        "\n",
        "        Inputs:\n",
        "            text_a: 'the dog is hairy .'\n",
        "        Tokenization:\n",
        "            text_a: 'the dog is hairy .'\n",
        "        Processed:\n",
        "            text_a: '[CLS] the dog is hairy . [SEP]'\n",
        "            type_ids: 0     0   0   0  0     0 0\n",
        "            valid_length: 7\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        line: tuple of str\n",
        "            Input strings. For sequence pairs, the input is a tuple of 2 strings:\n",
        "            (text_a, text_b). For single sequences, the input is a tuple of single\n",
        "            string: (text_a,).\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        np.array: input token ids in 'int32', shape (batch_size, seq_length)\n",
        "        np.array: valid length in 'int32', shape (batch_size,)\n",
        "        np.array: input token type ids in 'int32', shape (batch_size, seq_length)\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        # convert to unicode\n",
        "        text_a = line[0]\n",
        "        if self._pair:\n",
        "            assert len(line) == 2\n",
        "            text_b = line[1]\n",
        "\n",
        "        tokens_a = self._tokenizer.tokenize(text_a)\n",
        "        tokens_b = None\n",
        "\n",
        "        if self._pair:\n",
        "            tokens_b = self._tokenizer(text_b)\n",
        "\n",
        "        if tokens_b:\n",
        "            # Modifies `tokens_a` and `tokens_b` in place so that the total\n",
        "            # length is less than the specified length.\n",
        "            # Account for [CLS], [SEP], [SEP] with \"- 3\"\n",
        "            self._truncate_seq_pair(tokens_a, tokens_b,\n",
        "                                    self._max_seq_length - 3)\n",
        "        else:\n",
        "            # Account for [CLS] and [SEP] with \"- 2\"\n",
        "            if len(tokens_a) > self._max_seq_length - 2:\n",
        "                tokens_a = tokens_a[0:(self._max_seq_length - 2)]\n",
        "\n",
        "        # The embedding vectors for `type=0` and `type=1` were learned during\n",
        "        # pre-training and are added to the wordpiece embedding vector\n",
        "        # (and position vector). This is not *strictly* necessary since\n",
        "        # the [SEP] token unambiguously separates the sequences, but it makes\n",
        "        # it easier for the model to learn the concept of sequences.\n",
        "\n",
        "        # For classification tasks, the first vector (corresponding to [CLS]) is\n",
        "        # used as as the \"sentence vector\". Note that this only makes sense because\n",
        "        # the entire model is fine-tuned.\n",
        "        #vocab = self._tokenizer.vocab\n",
        "        vocab = self._vocab\n",
        "        tokens = []\n",
        "        tokens.append(vocab.cls_token)\n",
        "        tokens.extend(tokens_a)\n",
        "        tokens.append(vocab.sep_token)\n",
        "        segment_ids = [0] * len(tokens)\n",
        "\n",
        "        if tokens_b:\n",
        "            tokens.extend(tokens_b)\n",
        "            tokens.append(vocab.sep_token)\n",
        "            segment_ids.extend([1] * (len(tokens) - len(segment_ids)))\n",
        "\n",
        "        input_ids = self._tokenizer.convert_tokens_to_ids(tokens)\n",
        "\n",
        "        # The valid length of sentences. Only real  tokens are attended to.\n",
        "        valid_length = len(input_ids)\n",
        "\n",
        "        if self._pad:\n",
        "            # Zero-pad up to the sequence length.\n",
        "            padding_length = self._max_seq_length - valid_length\n",
        "            # use padding tokens for the rest\n",
        "            input_ids.extend([vocab[vocab.padding_token]] * padding_length)\n",
        "            segment_ids.extend([0] * padding_length)\n",
        "\n",
        "        return np.array(input_ids, dtype='int32'), np.array(valid_length, dtype='int32'),\\\n",
        "            np.array(segment_ids, dtype='int32')\n",
        "\n",
        "class BERTDataset(Dataset):\n",
        "    def __init__(self, dataset, sent_idx, label_idx, bert_tokenizer, vocab, max_len,\n",
        "                 pad, pair):\n",
        "        transform = BERTSentenceTransform(bert_tokenizer, max_seq_length=max_len,vocab=vocab, pad=pad, pair=pair)\n",
        "        #transform = nlp.data.BERTSentenceTransform(\n",
        "        #    tokenizer, max_seq_length=max_len, pad=pad, pair=pair)\n",
        "        self.sentences = [transform([i[sent_idx]]) for i in dataset]\n",
        "        self.labels = [np.int32(i[label_idx]) for i in dataset]\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        return (self.sentences[i] + (self.labels[i], ))\n",
        "\n",
        "    def __len__(self):\n",
        "        return (len(self.labels))\n",
        "\n"
      ],
      "metadata": {
        "id": "SLeWVPUdS7Uk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_train = BERTDataset(dataset_train, 0, 1, tokenizer, vocab, max_len, True, False)\n",
        "data_test = BERTDataset(dataset_test, 0, 1, tokenizer, vocab, max_len, True, False)\n"
      ],
      "metadata": {
        "id": "K0lglL5dS9bd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataloader = torch.utils.data.DataLoader(data_train, batch_size=batch_size, num_workers=5)\n",
        "test_dataloader = torch.utils.data.DataLoader(data_test, batch_size=batch_size, num_workers=5)"
      ],
      "metadata": {
        "id": "tFMq8WUhXGgY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BERTClassifier(nn.Module):\n",
        "    def __init__(self,\n",
        "                 bert,\n",
        "                 hidden_size = 768,\n",
        "                 num_classes=7,   ##클래스 수 조정##\n",
        "                 dr_rate=None,\n",
        "                 params=None):\n",
        "        super(BERTClassifier, self).__init__()\n",
        "        self.bert = bert\n",
        "        self.dr_rate = dr_rate\n",
        "\n",
        "        self.classifier = nn.Linear(hidden_size , num_classes)\n",
        "        if dr_rate:\n",
        "            self.dropout = nn.Dropout(p=dr_rate)\n",
        "\n",
        "    def gen_attention_mask(self, token_ids, valid_length):\n",
        "        attention_mask = torch.zeros_like(token_ids)\n",
        "        for i, v in enumerate(valid_length):\n",
        "            attention_mask[i][:v] = 1\n",
        "        return attention_mask.float()\n",
        "\n",
        "    def forward(self, token_ids, valid_length, segment_ids):\n",
        "        attention_mask = self.gen_attention_mask(token_ids, valid_length)\n",
        "\n",
        "        _, pooler = self.bert(input_ids = token_ids, token_type_ids = segment_ids.long(), attention_mask = attention_mask.float().to(token_ids.device),return_dict=False)\n",
        "        if self.dr_rate:\n",
        "            out = self.dropout(pooler)\n",
        "        return self.classifier(out)"
      ],
      "metadata": {
        "id": "_Me9HiTeXQGZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#BERT 모델 불러오기\n",
        "model = BERTClassifier(bertmodel,  dr_rate=0.5).to(device)\n",
        "\n",
        "#optimizer와 schedule 설정\n",
        "no_decay = ['bias', 'LayerNorm.weight']\n",
        "optimizer_grouped_parameters = [\n",
        "    {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
        "    {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
        "]\n",
        "\n",
        "optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate)\n",
        "loss_fn = nn.CrossEntropyLoss() # 다중분류를 위한 대표적인 loss func\n",
        "\n",
        "t_total = len(train_dataloader) * num_epochs\n",
        "warmup_step = int(t_total * warmup_ratio)\n",
        "\n",
        "scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=warmup_step, num_training_steps=t_total)\n",
        "\n",
        "#정확도 측정을 위한 함수 정의\n",
        "def calc_accuracy(X,Y):\n",
        "    max_vals, max_indices = torch.max(X, 1)\n",
        "    train_acc = (max_indices == Y).sum().data.cpu().numpy()/max_indices.size()[0]\n",
        "    return train_acc\n",
        "\n",
        "train_dataloader"
      ],
      "metadata": {
        "id": "Tj5MCew9XUfw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_history=[]\n",
        "test_history=[]\n",
        "loss_history=[]\n",
        "for e in range(num_epochs):\n",
        "    train_acc = 0.0\n",
        "    test_acc = 0.0\n",
        "    model.train()\n",
        "    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm_notebook(train_dataloader)):\n",
        "        optimizer.zero_grad()\n",
        "        token_ids = token_ids.long().to(device)\n",
        "        segment_ids = segment_ids.long().to(device)\n",
        "        valid_length= valid_length\n",
        "        label = label.long().to(device)\n",
        "        out = model(token_ids, valid_length, segment_ids)\n",
        "\n",
        "        #print(label.shape,out.shape)\n",
        "        loss = loss_fn(out, label)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
        "        optimizer.step()\n",
        "        scheduler.step()  # Update learning rate schedule\n",
        "        train_acc += calc_accuracy(out, label)\n",
        "        if batch_id % log_interval == 0:\n",
        "            print(\"epoch {} batch id {} loss {} train acc {}\".format(e+1, batch_id+1, loss.data.cpu().numpy(), train_acc / (batch_id+1)))\n",
        "            train_history.append(train_acc / (batch_id+1))\n",
        "            loss_history.append(loss.data.cpu().numpy())\n",
        "    print(\"epoch {} train acc {}\".format(e+1, train_acc / (batch_id+1)))\n",
        "    #train_history.append(train_acc / (batch_id+1))\n",
        "\n",
        "    model.eval()\n",
        "    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm_notebook(test_dataloader)):\n",
        "        token_ids = token_ids.long().to(device)\n",
        "        segment_ids = segment_ids.long().to(device)\n",
        "        valid_length= valid_length\n",
        "        label = label.long().to(device)\n",
        "        out = model(token_ids, valid_length, segment_ids)\n",
        "        test_acc += calc_accuracy(out, label)\n",
        "    print(\"epoch {} test acc {}\".format(e+1, test_acc / (batch_id+1)))\n",
        "    test_history.append(test_acc / (batch_id+1))\n"
      ],
      "metadata": {
        "id": "KdPUj3MEXXTf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(predict_sentence):\n",
        "\n",
        "    data = [predict_sentence, '0']\n",
        "    dataset_another = [data]\n",
        "    another_test = BERTDataset(dataset_another, 0, 1, tokenizer, vocab, max_len, True, False)\n",
        "    test_dataloader = torch.utils.data.DataLoader(another_test, batch_size=batch_size, num_workers=5)\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(test_dataloader):\n",
        "        token_ids = token_ids.long().to(device)\n",
        "        segment_ids = segment_ids.long().to(device)\n",
        "\n",
        "        valid_length= valid_length\n",
        "        label = label.long().to(device)\n",
        "\n",
        "        out = model(token_ids, valid_length, segment_ids)\n",
        "\n",
        "\n",
        "        test_eval=[]\n",
        "        for i in out:\n",
        "            logits=i\n",
        "            logits = logits.detach().cpu().numpy()\n",
        "\n",
        "            if np.argmax(logits) == 0:\n",
        "                test_eval.append(\"공포가\")\n",
        "            elif np.argmax(logits) == 1:\n",
        "                test_eval.append(\"놀람이\")\n",
        "            elif np.argmax(logits) == 2:\n",
        "                test_eval.append(\"분노가\")\n",
        "            elif np.argmax(logits) == 3:\n",
        "                test_eval.append(\"슬픔이\")\n",
        "            elif np.argmax(logits) == 4:\n",
        "                test_eval.append(\"중립이\")\n",
        "            elif np.argmax(logits) == 5:\n",
        "                test_eval.append(\"행복이\")\n",
        "            elif np.argmax(logits) == 6:\n",
        "                test_eval.append(\"혐오가\")\n",
        "\n",
        "        print(\">> 입력하신 내용에서 \" + test_eval[0] + \" 느껴집니다.\")"
      ],
      "metadata": {
        "id": "f1BATNdoZxSy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#질문 무한반복하기! 0 입력시 종료\n",
        "end = 1\n",
        "while end == 1 :\n",
        "    sentence = input(\"하고싶은 말을 입력해주세요 : \")\n",
        "    if sentence == \"0\" :\n",
        "        break\n",
        "    predict(sentence)\n",
        "    print(\"\\n\")"
      ],
      "metadata": {
        "id": "Th4XgHscZ0pO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -al\n"
      ],
      "metadata": {
        "id": "qBIHJRnFdcEi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lscOOVebd1uw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}